{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f04d09a3",
      "metadata": {
        "id": "f04d09a3"
      },
      "source": [
        "# Character-level GPT-2 Training Notebook\n",
        "This notebook combines the model definition and training loop for a character-level GPT-2 model. You can use this notebook to train your model on a GPU platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "889ed7b6",
      "metadata": {
        "id": "889ed7b6",
        "outputId": "87001990-3c92-4823-9aff-f93d81b1664f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0aa4c98e",
      "metadata": {
        "id": "0aa4c98e"
      },
      "outputs": [],
      "source": [
        "# Model Definition (from model.py)\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "        assert self.head_dim * heads == embed_size, \"Embed size must be divisible by heads\"\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (0.5)), dim=3)\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.embed_size)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.ffn(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=128, num_layers=2, heads=2, dropout=0.1, forward_expansion=2, max_length=64):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "        self.max_length = max_length\n",
        "    def forward(self, x, mask=None):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
        "        token_embeds = self.token_embedding(x)\n",
        "        pos_embeds = self.position_embedding(positions)\n",
        "        x = self.dropout(token_embeds + pos_embeds)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x, x, mask)\n",
        "        out = self.fc_out(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3186705b",
      "metadata": {
        "id": "3186705b"
      },
      "outputs": [],
      "source": [
        "# Dataset Definition (from train.py)\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length=64, pad_token='[PAD]'):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab = sorted(set(text) | set([pad_token]))\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.vocab)}\n",
        "        self.pad_token = pad_token\n",
        "        self.pad_idx = self.char_to_idx[pad_token]\n",
        "        self.data = [self.char_to_idx[ch] for ch in text]\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_length]\n",
        "        y = self.data[idx + self.seq_length]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "59eb6ded",
      "metadata": {
        "id": "59eb6ded"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "# Place your data.txt file in the same directory as this notebook or update the path below.\n",
        "data_path = Path(\"data.txt\")\n",
        "with open(data_path, \"r\") as f:\n",
        "    data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "28b4a070",
      "metadata": {
        "id": "28b4a070"
      },
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "seq_length = 64\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "lr = 3e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_save_path = \"gpt2_char_model.pt\"\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = TextDataset(data, seq_length=seq_length)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Model\n",
        "model = GPT2(vocab_size=dataset.vocab_size, max_length=seq_length, embed_size=128, num_layers=2, heads=2, dropout=0.1, forward_expansion=2)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f8db8c8d",
      "metadata": {
        "id": "f8db8c8d",
        "outputId": "a5cfc1d4-26fa-4d1a-df1b-aad6b84bb0dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 39494/39494 [05:27<00:00, 120.53it/s, loss=1.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 2.2241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 39494/39494 [05:29<00:00, 119.73it/s, loss=1.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 1.8945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 39494/39494 [05:25<00:00, 121.32it/s, loss=1.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Loss: 1.7673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 39494/39494 [05:20<00:00, 123.18it/s, loss=1.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Loss: 1.7040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 39494/39494 [05:18<00:00, 123.81it/s, loss=1.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Loss: 1.6650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 39494/39494 [05:19<00:00, 123.58it/s, loss=1.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Loss: 1.6370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 39494/39494 [05:19<00:00, 123.44it/s, loss=1.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Loss: 1.6168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 39494/39494 [05:21<00:00, 122.72it/s, loss=1.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Loss: 1.5998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 39494/39494 [05:20<00:00, 123.16it/s, loss=1.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Loss: 1.5858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 39494/39494 [05:21<00:00, 122.91it/s, loss=1.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Loss: 1.5744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  15%|█▌        | 5972/39494 [00:13<01:18, 426.58it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-54fd3838d6b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0meval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0meval_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "    for x, y in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        logits = logits[:, -1, :]\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'vocab': dataset.vocab,\n",
        "        'char_to_idx': dataset.char_to_idx,\n",
        "        'idx_to_char': dataset.idx_to_char\n",
        "    }, model_save_path)\n",
        "    # Evaluation after 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm.tqdm(train_loader, desc=\"Evaluating\"):\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                logits = model(x)\n",
        "                logits = logits[:, -1, :]\n",
        "                loss = criterion(logits, y)\n",
        "                eval_loss += loss.item() * x.size(0)\n",
        "        eval_loss /= len(dataset)\n",
        "        print(f\"Evaluation Loss after {epoch} epochs: {eval_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4757747c",
      "metadata": {
        "id": "4757747c"
      },
      "source": [
        "# Notes\n",
        "- Make sure your `data.txt` file is present in the same directory as this notebook.\n",
        "- Adjust hyperparameters as needed for your GPU platform.\n",
        "- The model and optimizer state will be saved to `gpt2_char_model.pt` after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a function that now laods the model and then create f unction to genearte from the modle and print the ouput of the model\n",
        "\n",
        "def load_model(model_path, vocab_size, seq_length):\n",
        "    \"\"\"Loads the trained model.\"\"\"\n",
        "    model = GPT2(vocab_size=vocab_size, max_length=seq_length, embed_size=128, num_layers=2, heads=2, dropout=0.1, forward_expansion=2)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    vocab = checkpoint['vocab']\n",
        "    char_to_idx = checkpoint['char_to_idx']\n",
        "    idx_to_char = checkpoint['idx_to_char']\n",
        "    return model, vocab, char_to_idx, idx_to_char\n",
        "\n",
        "def generate_text(model, start_string, char_to_idx, idx_to_char, num_generate=10000, temperature=1.0, seq_length=64):\n",
        "    \"\"\"Generates text using the trained model.\"\"\"\n",
        "    # Convert start string to indices\n",
        "    input_eval = [char_to_idx[s] for s in start_string]\n",
        "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Empty list to store generated text\n",
        "    text_generated = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            # Pass the current sequence into the model\n",
        "            # Use the last 'seq_length' characters as input if input_eval is longer\n",
        "            input_tensor = input_eval[:, max(0, input_eval.size(1) - seq_length):]\n",
        "\n",
        "            predictions = model(input_tensor)\n",
        "\n",
        "            # Remove the batch dimension and select the last character's predictions\n",
        "            predictions = predictions.squeeze(0)[-1, :]\n",
        "\n",
        "            # Apply temperature to control randomness\n",
        "            predictions = predictions / temperature\n",
        "            predicted_id = torch.multinomial(F.softmax(predictions, dim=0), num_samples=1).item()\n",
        "\n",
        "            # Pass the predicted character back into the input for the next step\n",
        "            input_eval = torch.cat([input_eval, torch.tensor([[predicted_id]], dtype=torch.long).to(device)], dim=1)\n",
        "\n",
        "            text_generated.append(idx_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have trained the model and saved it to 'gpt2_char_model.pt'\n",
        "# You also need the vocab size and seq_length from the training setup\n",
        "try:\n",
        "    # Load the model\n",
        "    loaded_model, vocab, char_to_idx, idx_to_char = load_model(model_save_path, dataset.vocab_size, seq_length)\n",
        "\n",
        "    # Generate text\n",
        "    start_prompt = \"The quick brown fox\"\n",
        "    generated_text = generate_text(loaded_model, start_prompt, char_to_idx, idx_to_char, num_generate=200)\n",
        "\n",
        "    # Print the generated text\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(generated_text)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Model file not found at {model_save_path}. Please train the model first.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "2mPOEIFzT82N",
        "outputId": "eeaee403-ef30-45e3-919e-002cfcaa2612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2mPOEIFzT82N",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "The quick brown foxi iN'sG's\n",
            " 'LiinBsi'isisGeseMtyB\n",
            ",'iGiB's',cis, I wrens him!\n",
            "  GATHAT alot nexut these ger\n",
            "    What'st o' batrly their with and hortain?\n",
            "    My think\n",
            "                                        Enter KE.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AM1ayr9xUqVF"
      },
      "id": "AM1ayr9xUqVF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}