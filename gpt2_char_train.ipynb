{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04d09a3",
   "metadata": {},
   "source": [
    "# Character-level GPT-2 Training Notebook\n",
    "This notebook combines the model definition and training loop for a character-level GPT-2 model. You can use this notebook to train your model on a GPU platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ed7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition (from model.py)\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (0.5)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.embed_size)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.ffn(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, num_layers=2, heads=2, dropout=0.1, forward_expansion=2, max_length=64):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.max_length = max_length\n",
    "    def forward(self, x, mask=None):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        pos_embeds = self.position_embedding(positions)\n",
    "        x = self.dropout(token_embeds + pos_embeds)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, mask)\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Definition (from train.py)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=64, pad_token='[PAD]'):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = sorted(set(text) | set([pad_token]))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.vocab)}\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_idx = self.char_to_idx[pad_token]\n",
    "        self.data = [self.char_to_idx[ch] for ch in text]\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Place your data.txt file in the same directory as this notebook or update the path below.\n",
    "data_path = Path(\"data.txt\")\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "seq_length = 64\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "lr = 3e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_save_path = \"gpt2_char_model.pt\"\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = TextDataset(data, seq_length=seq_length)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Model\n",
    "model = GPT2(vocab_size=dataset.vocab_size, max_length=seq_length, embed_size=128, num_layers=2, heads=2, dropout=0.1, forward_expansion=2)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for x, y in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        logits = logits[:, -1, :]\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'vocab': dataset.vocab,\n",
    "        'char_to_idx': dataset.char_to_idx,\n",
    "        'idx_to_char': dataset.idx_to_char\n",
    "    }, model_save_path)\n",
    "    # Evaluation after 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm.tqdm(train_loader, desc=\"Evaluating\"):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                logits = model(x)\n",
    "                logits = logits[:, -1, :]\n",
    "                loss = criterion(logits, y)\n",
    "                eval_loss += loss.item() * x.size(0)\n",
    "        eval_loss /= len(dataset)\n",
    "        print(f\"Evaluation Loss after {epoch} epochs: {eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757747c",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- Make sure your `data.txt` file is present in the same directory as this notebook.\n",
    "- Adjust hyperparameters as needed for your GPU platform.\n",
    "- The model and optimizer state will be saved to `gpt2_char_model.pt` after each epoch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
